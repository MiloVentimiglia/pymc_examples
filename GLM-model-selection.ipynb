{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PyMC3 Examples\n",
    "\n",
    "# GLM Model Selection using Bayes Factor\n",
    "\n",
    "\n",
    "**A minimal reproducable example of Model Selection using Bayes Factor.**\n",
    "\n",
    "+ This example evaluates two different regression models and declares which has the better fit according to a Bayes Factor comparison.\n",
    "+ The example is adapted specifically from Jake Vanderplas' [recent blogpost](https://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/) on model selection, for which he used an emcee sampler and numpy-based likelihood. The main purpose of this Notebook is to create a PyMC3-based version.\n",
    "+ See also this question on [Cross Validated](http://stats.stackexchange.com/questions/161082/bayesian-model-selection-in-pymc3) \n",
    "+ The dataset is tiny and generated within this Notebook. It contains errors in the measured value (y) only.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "\n",
    "+ Python 3.4 project using latest available [PyMC3](https://github.com/pymc-devs/pymc3)\n",
    "+ Developed using [ContinuumIO Anaconda](https://www.continuum.io/downloads) distribution on a Macbook Pro 3GHz i7, 16GB RAM, OSX 10.10.5.  \n",
    "+ Finally, if runs become unstable or Theano throws weird errors, try clearing the cache `$> theano-cache clear` and rerunning the notebook.\n",
    "\n",
    "\n",
    "**Package Requirements (shown as a conda-env YAML):**\n",
    "```\n",
    "$> less conda_env_pymc3_examples.yml\n",
    "\n",
    "name: pymc3_examples\n",
    "    channels:\n",
    "      - defaults\n",
    "    dependencies:\n",
    "      - python=3.4\n",
    "      - ipython\n",
    "      - ipython-notebook\n",
    "      - ipython-qtconsole\n",
    "      - numpy\n",
    "      - scipy\n",
    "      - matplotlib\n",
    "      - pandas\n",
    "      - seaborn\n",
    "      - patsy  \n",
    "      - pip\n",
    "\n",
    "$> conda env create --file conda_env_pymc3_examples.yml\n",
    "\n",
    "$> source activate pymc3_examples\n",
    "\n",
    "$> pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%qtconsole --colors=linux\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import fmin_powell\n",
    "import pymc3 as pm\n",
    "import theano as thno\n",
    "import theano.tensor as T \n",
    "\n",
    "from IPython.html.widgets import interactive, fixed\n",
    "\n",
    "# configure some basic options\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\")\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "rndst = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(n=20, p=0, a=1, b=1, c=0, latent_sigma_y=20):\n",
    "    ''' \n",
    "    Create an example dataset based on a very simple model that we might\n",
    "    imagine is a noisy physical process:\n",
    "        1. random x values within a range\n",
    "        2. latent error aka inherent noise in y\n",
    "        3. optional outliers from unknown sources\n",
    "\n",
    "    Model form: y ~ a + bx + cx^2 + e\n",
    "    \n",
    "    NOTE: latent_sigma_y is used to create a normally distributed,\n",
    "    'latent error' aka 'inherent noise' in the 'physical process' \n",
    "    generating thses values, rather than experimental measurement error. \n",
    "    Please don't use the returned `latent_error` values in inferential \n",
    "    models, it's there for interest only.\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame({'x':rndst.randint(0,100,n)})\n",
    "    df['latent_error'] = rndst.normal(0,latent_sigma_y,n)\n",
    "    df['outlier'] = rndst.binomial(1,p,n)\n",
    "\n",
    "    bimdl = np.append(rndst.normal(-.4,0.1,100), np.random.normal(.4,0.1,100))\n",
    "    df['outlier_adj'] = rndst.choice(bimdl, n, replace=False)\n",
    "                \n",
    "    ## create linear model\n",
    "    df['y'] = a + b*(df['x']) + c*(df['x'])**2 + df['latent_error']\n",
    "   \n",
    "    ## add extreme noise for outliers\n",
    "    df['y'] = df['y'] + (df['outlier'] * df['outlier_adj'] * df['y'])\n",
    "\n",
    "    ## round and return\n",
    "    for col in ['y','latent_error','x','outlier_adj']:\n",
    "        df[col] = np.round(df[col],3)\n",
    "       \n",
    "    return df\n",
    "\n",
    "\n",
    "def sketch_data(n=20, p=0, a=-30, b=5, c=0, latent_sigma_y=20):\n",
    "    ''' Sketch the generated data '''\n",
    "    \n",
    "    df = generate_data(n, p, a, b, c, latent_sigma_y)\n",
    "    ordr = 'linear' if c == 0 else 'quadratic'\n",
    "    \n",
    "    g = sns.FacetGrid(df, size=8, hue='outlier', hue_order=[True,False],\n",
    "                  palette='Set1', legend_out=False)\n",
    "\n",
    "    _ = g.map(plt.errorbar, 'x', 'y', 'latent_error', marker=\"o\"\n",
    "              , ls='', elinewidth=0.7).add_legend()\n",
    "\n",
    "    plotx = np.linspace(df['x'].min() - np.ptp(df['x'])*.1\n",
    "                        ,df['x'].max() + np.ptp(df['x'])*.1, 100)\n",
    "    ploty = a + b*plotx + c*plotx**2\n",
    "\n",
    "    _ = plt.plot(plotx,ploty,'--',alpha=0.8)\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    _ = g.fig.suptitle('Sketch of Data Generation ({})'.format(ordr)\n",
    "                       ,fontsize=16)\n",
    "    \n",
    "\n",
    "def plot_traces(traces, retain=1000):\n",
    "    ''' Convenience function for plotting traces with overlaid means and values\n",
    "    '''\n",
    "    \n",
    "    ax = pm.traceplot(traces[-retain:], figsize=(12,len(traces.varnames)*1.5),\n",
    "        lines={k: v['mean'] for k, v in pm.df_summary(traces[-retain:]).iterrows()})\n",
    "\n",
    "    for i, mn in enumerate(pm.df_summary(traces[-kp:])['mean']):\n",
    "        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data'\n",
    "                    ,xytext=(5,10), textcoords='offset points', rotation=90\n",
    "                    ,va='bottom', fontsize='large', color='#AA0022')\n",
    "\n",
    "    \n",
    "def create_poly_modelspec(k=1):\n",
    "    ''' Convenience function to create a polynomial modelspec string \n",
    "    '''\n",
    "    return ('y ~ 1 + x ' + ' '.join(['+ np.power(x,{})'.format(j) for j in range(2,k+1)])).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a dummy dataset based on a specific distribution, so we can better evaluate the correctness of the automated model selection later.\n",
    "\n",
    "Use the interactive plot below to get a feel for the possibilities of data we could generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interactive(sketch_data, n=[5,50,5], p=[0,.5,.05], a=[-50,50], b=[-10,10], c=[-10,10]\n",
    "            ,latent_sigma_y=[0,100,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ I've shown the `latent_error` in errorbars, but this is for interest only, since this shows the _inherent noise_ in whatever 'physical process' we imagine created the data.\n",
    "+ There is no _measurement error_.\n",
    "+ Datapoints created as outliers are shown in **red**, again for interest only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above interactive plot to get a feel for the effect of the params. Now I'll create a fixed dataset to use for the remainder of the Notebook. \n",
    "\n",
    "For a start, I'll create a linear model with no outliers. Keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = generate_data(n=10, p=0, a=-30, b=5, latent_sigma_y=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(x='x', y='y', data=df, fit_reg=False, size=8\n",
    "               ,scatter_kws={'alpha':0.7,'s':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ This then, is our dataset for the remainder of the notebook. Let's see how well we can fit it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = df.copy()\n",
    "dfs['x'] = (df['x'] - df['x'].mean()) / df['x'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create ranges for later ylim xim\n",
    "dfs_ylims = (dfs['y'].min() - np.ptp(dfs['y'])/10,dfs['y'].max() + np.ptp(dfs['y'])/10)\n",
    "dfs_xlims = (dfs['x'].min() - np.ptp(dfs['x'])/10,dfs['x'].max() + np.ptp(dfs['x'])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Simple OLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *linear model* is really simple and conventional:\n",
    "$$y = a + bx + \\epsilon$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model using ordinary pymc3 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl_ols:\n",
    "        \n",
    "    ## define Normal priors to give Ridge regression\n",
    "    b0 = pm.Normal('b0', mu=0, sd=100)\n",
    "    b1 = pm.Normal('b1', mu=0, sd=100)\n",
    " \n",
    "    ## define Linear model\n",
    "    yest = b0 + b1 * df['x']\n",
    "\n",
    "    ## define Normal likelihood with HalfCauchy noise (fat tails, equiv HalfT 1DoF)\n",
    "    sigma_y = pm.HalfCauchy('sigma_y', beta=10)\n",
    "    likelihood = pm.Normal('likelihood', mu=yest, sd=sigma_y, observed=df['y'])\n",
    "\n",
    "    ## sample using NUTS (starting from MAP found using powell)\n",
    "    start_MAP = pm.find_MAP(fmin=fmin_powell, disp=True)\n",
    "    traces_ols = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View Traces after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_traces(traces_ols, retain=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ This simple OLS manages to make fairly good guesses - the data has been generated fairly simply after all - but it does appear to have been fooled by the inherent noise and outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model using pymc3 GLM method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3 has a quite recently developed method - `glm` - for defining models using a `patsy`-style formula syntax. This seems really useful, especially for defining simple regression models in fewer lines of code. \n",
    "\n",
    "I couldn't find a direct comparison in the the examples, so before I launch into using `glm` for the rest of the Notebook, here's the same OLS model as above, defined using `glm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mdl_ols_glm:\n",
    "\n",
    "    # setup model with Normal likelihood (which uses HalfCauchy for error prior)\n",
    "    pm.glm.glm('y ~ 1 + x', df, family=pm.glm.families.Normal())\n",
    "    \n",
    "    ## sample using NUTS (starting from MAP found using powell)\n",
    "    start_MAP = pm.find_MAP(fmin=fmin_powell, disp=True)\n",
    "    traces_ols_glm = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View Traces after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_traces(traces_ols_glm, retain=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ The output parameters are of course named differently to my custom naming before. Now we have:\n",
    "\n",
    "    `b0 == Intercept`  \n",
    "    `b1 == x`  \n",
    "    `sigma_y_log == sd_log`  \n",
    "    `sigma_y == sd`  \n",
    "    \n",
    "    \n",
    "+ However, naming aside, this `glm`-defined model appears to behave in a very similar way, and find pretty much the same parameter values as the conventionally-defined model - any differences are due to the random nature of the sampling.\n",
    "+ I'll quite happily use the `glm` syntax for further models below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Higher-Order OLS Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the real purpose of this Notebook: demonstrate model selection using Bayes Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models, traces = OrderedDict(), OrderedDict()\n",
    "\n",
    "for k in range(1,8):\n",
    "\n",
    "    nm = 'k{}'.format(k)\n",
    "    fml = create_poly_modelspec(k)\n",
    "    \n",
    "    with pm.Model() as models[nm]:\n",
    "\n",
    "        print('\\nRunning: {}'.format(nm))\n",
    "        # setup model with Normal likelihood (which uses HalfCauchy for error prior)\n",
    "        pm.glm.glm(fml, dfs, family=pm.glm.families.Normal())\n",
    "\n",
    "        ## sample using NUTS (starting from MAP found using powell)\n",
    "        start_MAP = pm.find_MAP(fmin=fmin_powell, disp=False)\n",
    "        traces[nm] = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get negative log likelioods straight from model.logp\n",
    "\n",
    "loglks = OrderedDict()\n",
    "\n",
    "for nm, mdl in models.items():\n",
    "    loglks[nm] = -mdl.logp(pm.df_summary(traces[nm])['mean'].to_dict())\n",
    "\n",
    "dfll = pd.DataFrame.from_dict(loglks, orient='index')\n",
    "dfll.rename(columns={0:'loglikelihood'}, inplace=True)\n",
    "ax = dfll.plot(kind='bar', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_traces(traces['k2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Posterior Predictive Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(x='x', y='y', data=dfs, fit_reg=False, size=8\n",
    "               ,scatter_kws={'alpha':0.7,'s':100})\n",
    "\n",
    "def lm(x, s):\n",
    "    return s['Intercept'] \\\n",
    "            + s['x']*x \\\n",
    "            + s['np.power(x, 2)']*np.power(x,2) \\\n",
    "            + s['np.power(x, 3)']*np.power(x,3) \\\n",
    "            + s['np.power(x, 4)']*np.power(x,4) \\\n",
    "            + s['np.power(x, 5)']*np.power(x,5)\n",
    "\n",
    "pm.glm.plot_posterior_predictive(traces['k5'][-1000:]\n",
    "        ,eval=np.linspace(dfs_xlims[0], dfs_xlims[1], 200)\n",
    "        ,lm=lm, samples=500, color='#00AA00', alpha=.2)\n",
    "\n",
    "_ = g.axes[0][0].set_ylim(dfs_ylims)\n",
    "_ = g.axes[0][0].set_xlim(dfs_xlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, size=8, hue='outlier', hue_order=[True,False],\n",
    "                  palette='Set1', legend_out=False)\n",
    "\n",
    "lm = lambda x, samp: samp['b0'] + samp['b1'] * x\n",
    "xrng = df['x'].max() - df['x'].min()\n",
    "\n",
    "pm.glm.plot_posterior_predictive(traces_ols[-1000:]\n",
    "        ,eval=np.linspace(df['x'].min() - np.ptp(df['x'])/10\n",
    "                        ,df['x'].max() + np.ptp(df['x'])/10, 10)\n",
    "        ,lm=lm, samples=200, color='#00AA00', alpha=.1)\n",
    "\n",
    "_ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', marker=\"o\", ls='').add_legend()\n",
    "\n",
    "\n",
    "plotx = np.linspace(df['x'].min() - np.ptp(df['x'])/10\n",
    "                        ,df['x'].max() + np.ptp(df['x'])/10, 10)\n",
    "ploty = -30 + 5* plotx\n",
    "\n",
    "_ = plt.plot(plotx,ploty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['sigma_y'].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example originally contributed by Jonathan Sedar 2015-12-30 [github.com/jonsedar](https://github.com/jonsedar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
